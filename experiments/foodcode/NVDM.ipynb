{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import sys\n",
    "sys.path.insert(0, '../../preprocess')\n",
    "import vectorizer\n",
    "vec = cPickle.load(open('../../yelpdata/total_vec_120K_embed.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "ds = pd.read_csv('../../yelpdata/total_data_120K.csv')\n",
    "aspect_columns = ['bit', 'domain']\n",
    "                 \n",
    "ds['bit'] = ds['stars'].apply(lambda x : x > 3.0)\n",
    "train_idxs, val_idxs = train_test_split(ds.index, stratify=ds[['bit', 'domain']], train_size=0.9, random_state=1337)\n",
    "\n",
    "idxs = val_idxs\n",
    "H = {}\n",
    "for aspect in aspect_columns :\n",
    "    H[aspect] = np.zeros((len(idxs), len(idxs)))\n",
    "    aspect_vals = ds[aspect].unique()\n",
    "    for val in aspect_vals :\n",
    "        a = set(ds[ds[aspect] == val].index) & set(idxs)\n",
    "        a = map(lambda s : list(idxs).index(s), a)\n",
    "        for j in a :\n",
    "            H[aspect][j, a] = 1\n",
    "\n",
    "    H[aspect][np.arange(len(idxs)), np.arange(len(idxs))] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = vec.X[train_idxs]\n",
    "X_tf = np.zeros((train_X.shape[0], vec.vocab_size))\n",
    "for i in range(len(train_X)) :\n",
    "    X_tf[i, train_X[i, :]] = 1.\n",
    "\n",
    "X_tf = X_tf[:, 2:]\n",
    "train_Xtf = X_tf\n",
    "\n",
    "val_X = vec.X[val_idxs]\n",
    "X_tf = np.zeros((val_X.shape[0], vec.vocab_size))\n",
    "for i in range(len(val_X)) :\n",
    "    X_tf[i, val_X[i, :]] = 1.\n",
    "\n",
    "X_tf = X_tf[:, 2:]\n",
    "val_Xtf = X_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 5e-5\n",
    "batch_size = 64\n",
    "vocab_size = vec.vocab_size - 2\n",
    "intermediate_dim = 500\n",
    "latent_dim = 200\n",
    "epochs = 1000\n",
    "epsilon_std = 1.0\n",
    "activation = 'tanh'\n",
    "\n",
    "x = Input(shape=(vocab_size,), name='x')\n",
    "h = Dense(intermediate_dim, activation=activation, name='h')(x)\n",
    "mu = Dense(latent_dim, name='mu')(h)\n",
    "log_sigma2 = Dense(latent_dim, name='l')(h)\n",
    "encoder = Model(x, mu)\n",
    "\n",
    "# reparameterized sampler for normal distributions\n",
    "def sample_norm(args):\n",
    "    '''reparameterized sampling from normal distribution'''\n",
    "    mu, log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(mu)[0], latent_dim,), mean=0.)\n",
    "    return mu + K.exp(0.5 * log_var) * epsilon\n",
    "\n",
    "# decoder / generative network\n",
    "z = Lambda(sample_norm, output_shape=(latent_dim,), name='z')([mu, log_sigma2])\n",
    "e = Dense(vocab_size, name='e')(z)\n",
    "\n",
    "def log_softmax(x, axis=None):\n",
    "    x0 = x - K.max(x, axis=axis, keepdims=True)\n",
    "    log_sum_exp_x0 = K.log(K.sum(K.exp(x0), axis=axis, keepdims=True))\n",
    "    return x0 - log_sum_exp_x0\n",
    "\n",
    "def kl_loss(x, e): \n",
    "    return (- 0.5 * K.sum(1 + log_sigma2 - K.square(mu) - K.exp(log_sigma2), axis=-1))\n",
    "\n",
    "\n",
    "def cross_ent_loss(x, e): \n",
    "    return - K.sum(x * log_softmax(e, axis=-1), axis=-1) \n",
    "    \n",
    "\n",
    "def vae_loss(x, e):\n",
    "    xent_loss = cross_ent_loss(x, e)\n",
    "    kld = kl_loss(x, e)\n",
    "    return xent_loss + kld\n",
    "\n",
    "\n",
    "opt = optimizers.adam(lr=learning_rate)\n",
    "vae = Model(x, e)\n",
    "vae.compile(optimizer=opt, \n",
    "            loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 102941 samples, validate on 11438 samples\n",
      "Epoch 1/1000\n",
      "102941/102941 [==============================] - 30s - loss: 698.1495 - val_loss: 669.9227\n",
      "Epoch 2/1000\n",
      "102941/102941 [==============================] - 30s - loss: 657.9908 - val_loss: 661.1227\n",
      "Epoch 3/1000\n",
      "102941/102941 [==============================] - 29s - loss: 651.8849 - val_loss: 656.9079\n",
      "Epoch 4/1000\n",
      "102941/102941 [==============================] - 29s - loss: 648.6019 - val_loss: 654.6730\n",
      "Epoch 5/1000\n",
      "102941/102941 [==============================] - 28s - loss: 646.5958 - val_loss: 652.8051\n",
      "Epoch 6/1000\n",
      "102941/102941 [==============================] - 30s - loss: 645.2503 - val_loss: 651.7027\n",
      "Epoch 7/1000\n",
      "102941/102941 [==============================] - 32s - loss: 644.2501 - val_loss: 650.7051\n",
      "Epoch 8/1000\n",
      "102941/102941 [==============================] - 31s - loss: 643.2128 - val_loss: 650.0488\n",
      "Epoch 9/1000\n",
      "102941/102941 [==============================] - 31s - loss: 642.4462 - val_loss: 649.1178\n",
      "Epoch 10/1000\n",
      "102941/102941 [==============================] - 31s - loss: 641.7368 - val_loss: 649.0128\n",
      "Epoch 11/1000\n",
      "102941/102941 [==============================] - 31s - loss: 641.1493 - val_loss: 647.9966\n",
      "Epoch 12/1000\n",
      "102941/102941 [==============================] - 31s - loss: 640.5028 - val_loss: 647.5583\n",
      "Epoch 13/1000\n",
      "102941/102941 [==============================] - 31s - loss: 640.0091 - val_loss: 646.9952\n",
      "Epoch 14/1000\n",
      "102941/102941 [==============================] - 31s - loss: 639.4528 - val_loss: 646.5354\n",
      "Epoch 15/1000\n",
      "102941/102941 [==============================] - 30s - loss: 638.9511 - val_loss: 645.8502\n",
      "Epoch 16/1000\n",
      "102941/102941 [==============================] - 30s - loss: 638.4200 - val_loss: 645.5961\n",
      "Epoch 17/1000\n",
      "102941/102941 [==============================] - 30s - loss: 637.9995 - val_loss: 645.2020\n",
      "Epoch 18/1000\n",
      "102941/102941 [==============================] - 30s - loss: 637.5999 - val_loss: 644.9925\n",
      "Epoch 19/1000\n",
      "102941/102941 [==============================] - 30s - loss: 637.2367 - val_loss: 644.1809\n",
      "Epoch 20/1000\n",
      "102941/102941 [==============================] - 30s - loss: 636.7470 - val_loss: 644.0650\n",
      "Epoch 21/1000\n",
      "102941/102941 [==============================] - 30s - loss: 636.4648 - val_loss: 643.9361\n",
      "Epoch 22/1000\n",
      "102941/102941 [==============================] - 30s - loss: 635.9939 - val_loss: 643.3344\n",
      "Epoch 23/1000\n",
      "102941/102941 [==============================] - 31s - loss: 635.7684 - val_loss: 642.8429\n",
      "Epoch 24/1000\n",
      "102941/102941 [==============================] - 30s - loss: 635.3938 - val_loss: 642.7107\n",
      "Epoch 25/1000\n",
      "102941/102941 [==============================] - 30s - loss: 635.0922 - val_loss: 642.2922\n",
      "Epoch 26/1000\n",
      "102941/102941 [==============================] - 30s - loss: 634.6889 - val_loss: 642.1258\n",
      "Epoch 27/1000\n",
      "102941/102941 [==============================] - 31s - loss: 634.4058 - val_loss: 641.7113\n",
      "Epoch 28/1000\n",
      " 84928/102941 [=======================>......] - ETA: 5s - loss: 632.9312"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6c59549ad0c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlyStopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         validation_split=0.1)\n\u001b[0m",
      "\u001b[0;32m/home/sarthak/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/home/sarthak/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1172\u001b[0m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                         raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m/home/sarthak/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_slice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "patience = 0\n",
    "earlyStopping = callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='min')\n",
    "\n",
    "vae.fit(train_Xtf,  \n",
    "        train_Xtf, \n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1, \n",
    "        callbacks=[earlyStopping], \n",
    "        validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedds = encoder.predict(val_Xtf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "embedds_n = normalize(embedds, 'l2')\n",
    "scores = np.dot(embedds_n, embedds_n.T)\n",
    "nb_studies = len(val_idxs)\n",
    "scores[np.arange(nb_studies), np.arange(nb_studies)] = -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bit 0.634551691273\n",
      "domain 0.788111027467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "scores = np.array(scores)\n",
    "for aspect_j in H :\n",
    "    aucs = [0] * nb_studies\n",
    "    for i in range(nb_studies) :\n",
    "        aucs[i] = roc_auc_score(H[aspect_j][i], scores[i])\n",
    "    print aspect_j, np.mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

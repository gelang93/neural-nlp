{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import sys\n",
    "sys.path.insert(0, '../../preprocess')\n",
    "import vectorizer\n",
    "beer_data = cPickle.load(open('../../beer_data/beer_vec_ds_df10.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "ds = pd.read_csv('../../beer_data/beer_ds.csv')\n",
    "\n",
    "from ast import literal_eval as make_tuple\n",
    "ds['bits'] = ds['bits'].map(lambda s : make_tuple(s))\n",
    "\n",
    "aspect_columns = sorted(['review/appearance', 'review/taste', 'review/aroma', 'review/palate'])\n",
    "\n",
    "train_idxs, val_idxs = train_test_split(ds.index, stratify=ds['bits'], train_size=0.9, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = beer_data.X[train_idxs]\n",
    "X_tf = np.zeros((train_X.shape[0], beer_data.vocab_size))\n",
    "for i in range(len(train_X)) :\n",
    "    X_tf[i, train_X[i, :]] = 1.\n",
    "\n",
    "X_tf = X_tf[:, 2:]\n",
    "train_Xtf = X_tf\n",
    "\n",
    "val_X = beer_data.X[val_idxs]\n",
    "X_tf = np.zeros((val_X.shape[0], beer_data.vocab_size))\n",
    "for i in range(len(val_X)) :\n",
    "    X_tf[i, val_X[i, :]] = 1.\n",
    "\n",
    "X_tf = X_tf[:, 2:]\n",
    "val_Xtf = X_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = val_idxs\n",
    "H = {}\n",
    "for i in range(4) :\n",
    "    H[str(i)] = np.zeros((len(idxs), len(idxs)))\n",
    "    a0 = set(ds[ds[aspect_columns[i]] == 0].index) & set(idxs)\n",
    "    a1 = set(ds[ds[aspect_columns[i]] == 1].index) & set(idxs)\n",
    "    a0 = map(lambda s : list(idxs).index(s), a0)\n",
    "    a1 = map(lambda s : list(idxs).index(s), a1)\n",
    "    for j in a0 :\n",
    "        H[str(i)][j, a0] = 1\n",
    "    for j in a1 :\n",
    "        H[str(i)][j, a1] = 1\n",
    "\n",
    "    H[str(i)][np.arange(len(idxs)), np.arange(len(idxs))] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.500170493183\n",
      "6 0.5831631426\n",
      "11 0.654411021299\n",
      "16 0.632510741257\n",
      "21 0.665045907498\n",
      "26 0.700779968083\n",
      "31 0.688440305402\n",
      "36 0.689175010013\n",
      "41 0.707412652339\n",
      "46 0.702140864486\n",
      "51 0.704337578871\n",
      "56 0.708874523588\n",
      "61 0.698482832347\n",
      "66 0.701969316004\n",
      "71 0.706314184499\n",
      "76 0.712376043307\n",
      "81 0.706971200091\n",
      "86 0.707239246642\n",
      "91 0.703389710955\n",
      "96 0.708253497004\n",
      "101 0.710427913085\n",
      "106 0.707307906607\n",
      "111 0.710253068616\n",
      "116 0.710623215004\n",
      "121 0.718300717019\n",
      "126 0.717827256605\n",
      "131 0.711738942091\n",
      "136 0.713365719461\n",
      "141 0.71196521616\n",
      "146 0.714044095257\n",
      "151 0.710920213292\n",
      "156 0.710111289099\n",
      "161 0.708467073387\n",
      "166 0.711255949706\n",
      "171 0.713655512221\n",
      "176 0.713832026132\n",
      "181 0.708481051749\n",
      "186 0.711625241759\n",
      "191 0.712755709526\n",
      "196 0.710443357611\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 5e-5\n",
    "batch_size = 128\n",
    "vocab_size = beer_data.vocab_size - 2\n",
    "intermediate_dim = 500\n",
    "latent_dim = 4\n",
    "epochs = 1000\n",
    "epsilon_std = 1.0\n",
    "activation = 'tanh'\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for n_top in range(1, 200, 5) :\n",
    "    latent_dim = n_top\n",
    "    x = Input(shape=(vocab_size,), name='x')\n",
    "    h = Dense(intermediate_dim, activation=activation, name='h')(x)\n",
    "    mu = Dense(latent_dim, name='mu')(h)\n",
    "    log_sigma2 = Dense(latent_dim, name='l')(h)\n",
    "    encoder = Model(x, mu)\n",
    "\n",
    "    # reparameterized sampler for normal distributions\n",
    "    def sample_norm(args):\n",
    "        '''reparameterized sampling from normal distribution'''\n",
    "        mu, log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(mu)[0], latent_dim,), mean=0.)\n",
    "        return mu + K.exp(0.5 * log_var) * epsilon\n",
    "\n",
    "    # decoder / generative network\n",
    "    z = Lambda(sample_norm, output_shape=(latent_dim,), name='z')([mu, log_sigma2])\n",
    "    e = Dense(vocab_size, name='e')(z)\n",
    "\n",
    "    def log_softmax(x, axis=None):\n",
    "        x0 = x - K.max(x, axis=axis, keepdims=True)\n",
    "        log_sum_exp_x0 = K.log(K.sum(K.exp(x0), axis=axis, keepdims=True))\n",
    "        return x0 - log_sum_exp_x0\n",
    "\n",
    "    def kl_loss(x, e): \n",
    "        return (- 0.5 * K.sum(1 + log_sigma2 - K.square(mu) - K.exp(log_sigma2), axis=-1))\n",
    "\n",
    "\n",
    "    def cross_ent_loss(x, e): \n",
    "        return - K.sum(x * log_softmax(e, axis=-1), axis=-1) \n",
    "\n",
    "\n",
    "    def vae_loss(x, e):\n",
    "        xent_loss = cross_ent_loss(x, e)\n",
    "        kld = kl_loss(x, e)\n",
    "        return xent_loss + kld\n",
    "\n",
    "\n",
    "    opt = optimizers.adam(lr=learning_rate)\n",
    "    vae = Model(x, e)\n",
    "    vae.compile(optimizer=opt, \n",
    "                loss=vae_loss)\n",
    "    from keras import callbacks\n",
    "    patience = 0\n",
    "    earlyStopping = callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='min')\n",
    "\n",
    "    vae.fit(train_Xtf,  \n",
    "            train_Xtf, \n",
    "            shuffle=True,\n",
    "            epochs=10,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0, \n",
    "            callbacks=[earlyStopping], \n",
    "            validation_split=0.1)\n",
    "    embedds = encoder.predict(val_Xtf)\n",
    "    from sklearn.preprocessing import normalize\n",
    "    embedds_n = normalize(embedds, 'l2')\n",
    "    scores = np.dot(embedds_n, embedds_n.T)\n",
    "    nb_studies = len(val_idxs)\n",
    "    scores[np.arange(nb_studies), np.arange(nb_studies)] = -1000\n",
    "    aucs = [0] * nb_studies\n",
    "    for i in range(nb_studies) :\n",
    "        aucs[i] = roc_auc_score(H['0'][i], scores[i])\n",
    "    print n_top, np.mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

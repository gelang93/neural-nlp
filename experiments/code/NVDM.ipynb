{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import sys\n",
    "sys.path.insert(0, '../../preprocess')\n",
    "import vectorizer\n",
    "\n",
    "vec = cPickle.load(open('../data/vectorizers/allfields_with_embedding_5000.p', 'rb'))\n",
    "cohen_vec = cPickle.load(open('../data/vectorizers/cohendata_dedup_5000.p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = vec.index['abstract']\n",
    "vec.X = vec.X[index[0]:index[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_X = vec.X\n",
    "X_tf = np.zeros((train_X.shape[0], vec.vocab_size))\n",
    "for i in range(len(train_X)) :\n",
    "    X_tf[i, train_X[i, :]] = 1.\n",
    "\n",
    "X_tf = X_tf[:, 2:]\n",
    "train_Xtf = X_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 5e-5\n",
    "batch_size = 64\n",
    "vocab_size = vec.vocab_size - 2\n",
    "intermediate_dim = 500\n",
    "latent_dim = 200\n",
    "epochs = 1000\n",
    "epsilon_std = 1.0\n",
    "activation = 'tanh'\n",
    "\n",
    "x = Input(shape=(vocab_size,), name='x')\n",
    "h = Dense(intermediate_dim, activation=activation, name='h')(x)\n",
    "mu = Dense(latent_dim, name='mu')(h)\n",
    "log_sigma2 = Dense(latent_dim, name='l')(h)\n",
    "encoder = Model(x, mu)\n",
    "\n",
    "# reparameterized sampler for normal distributions\n",
    "def sample_norm(args):\n",
    "    '''reparameterized sampling from normal distribution'''\n",
    "    mu, log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(mu)[0], latent_dim,), mean=0.)\n",
    "    return mu + K.exp(0.5 * log_var) * epsilon\n",
    "\n",
    "# decoder / generative network\n",
    "z = Lambda(sample_norm, output_shape=(latent_dim,), name='z')([mu, log_sigma2])\n",
    "e = Dense(vocab_size, name='e')(z)\n",
    "\n",
    "def log_softmax(x, axis=None):\n",
    "    x0 = x - K.max(x, axis=axis, keepdims=True)\n",
    "    log_sum_exp_x0 = K.log(K.sum(K.exp(x0), axis=axis, keepdims=True))\n",
    "    return x0 - log_sum_exp_x0\n",
    "\n",
    "def kl_loss(x, e): \n",
    "    return (- 0.5 * K.sum(1 + log_sigma2 - K.square(mu) - K.exp(log_sigma2), axis=-1))\n",
    "\n",
    "\n",
    "def cross_ent_loss(x, e): \n",
    "    return - K.sum(x * log_softmax(e, axis=-1), axis=-1) \n",
    "    \n",
    "\n",
    "def vae_loss(x, e):\n",
    "    xent_loss = cross_ent_loss(x, e)\n",
    "    kld = kl_loss(x, e)\n",
    "    return xent_loss + kld\n",
    "\n",
    "\n",
    "opt = optimizers.adam(lr=learning_rate)\n",
    "vae = Model(x, e)\n",
    "vae.compile(optimizer=opt, \n",
    "            loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37605 samples, validate on 4179 samples\n",
      "Epoch 1/1000\n",
      "37605/37605 [==============================] - 7s - loss: 700.8528 - val_loss: 695.0157\n",
      "Epoch 2/1000\n",
      "37605/37605 [==============================] - 7s - loss: 700.5169 - val_loss: 694.8646\n",
      "Epoch 3/1000\n",
      "37605/37605 [==============================] - 7s - loss: 700.1008 - val_loss: 694.0322\n",
      "Epoch 4/1000\n",
      "37605/37605 [==============================] - 7s - loss: 699.7725 - val_loss: 694.0046\n",
      "Epoch 5/1000\n",
      "37605/37605 [==============================] - 7s - loss: 699.2780 - val_loss: 693.4880\n",
      "Epoch 6/1000\n",
      "37605/37605 [==============================] - 7s - loss: 698.9924 - val_loss: 693.3763\n",
      "Epoch 7/1000\n",
      "37605/37605 [==============================] - 7s - loss: 698.6227 - val_loss: 693.2508\n",
      "Epoch 8/1000\n",
      "37605/37605 [==============================] - 7s - loss: 698.1155 - val_loss: 692.7852\n",
      "Epoch 9/1000\n",
      "37605/37605 [==============================] - 7s - loss: 697.7956 - val_loss: 692.8701\n",
      "Epoch 10/1000\n",
      "37605/37605 [==============================] - 7s - loss: 697.5147 - val_loss: 692.3420\n",
      "Epoch 11/1000\n",
      "37605/37605 [==============================] - 7s - loss: 697.0851 - val_loss: 692.3895\n",
      "Epoch 12/1000\n",
      "37605/37605 [==============================] - 7s - loss: 696.7953 - val_loss: 692.1688\n",
      "Epoch 13/1000\n",
      "37605/37605 [==============================] - 7s - loss: 696.5171 - val_loss: 691.8183\n",
      "Epoch 14/1000\n",
      "37605/37605 [==============================] - 7s - loss: 696.2125 - val_loss: 691.1952\n",
      "Epoch 15/1000\n",
      "37605/37605 [==============================] - 7s - loss: 695.8996 - val_loss: 690.8209\n",
      "Epoch 16/1000\n",
      "37605/37605 [==============================] - 7s - loss: 695.6593 - val_loss: 690.3236\n",
      "Epoch 17/1000\n",
      "37605/37605 [==============================] - 7s - loss: 695.3756 - val_loss: 690.6316\n",
      "Epoch 18/1000\n",
      "37605/37605 [==============================] - 7s - loss: 694.9850 - val_loss: 690.6084\n",
      "Epoch 19/1000\n",
      "37605/37605 [==============================] - 7s - loss: 694.7292 - val_loss: 690.4379\n",
      "Epoch 20/1000\n",
      "37605/37605 [==============================] - 7s - loss: 694.3963 - val_loss: 690.1617\n",
      "Epoch 21/1000\n",
      "37605/37605 [==============================] - 7s - loss: 694.0787 - val_loss: 689.9039\n",
      "Epoch 22/1000\n",
      "37605/37605 [==============================] - 7s - loss: 693.8352 - val_loss: 689.6809\n",
      "Epoch 23/1000\n",
      "37605/37605 [==============================] - 7s - loss: 693.6550 - val_loss: 689.3609\n",
      "Epoch 24/1000\n",
      "37605/37605 [==============================] - 7s - loss: 693.4131 - val_loss: 689.1730\n",
      "Epoch 25/1000\n",
      "37605/37605 [==============================] - 7s - loss: 693.1869 - val_loss: 688.7473\n",
      "Epoch 26/1000\n",
      "37605/37605 [==============================] - 7s - loss: 692.9389 - val_loss: 688.9379\n",
      "Epoch 27/1000\n",
      "37605/37605 [==============================] - 7s - loss: 692.6670 - val_loss: 688.6907\n",
      "Epoch 28/1000\n",
      "37605/37605 [==============================] - 7s - loss: 692.4699 - val_loss: 688.6972\n",
      "Epoch 29/1000\n",
      "37605/37605 [==============================] - 7s - loss: 692.3376 - val_loss: 688.3657\n",
      "Epoch 30/1000\n",
      "37605/37605 [==============================] - 7s - loss: 692.0612 - val_loss: 688.0623\n",
      "Epoch 31/1000\n",
      "37605/37605 [==============================] - 7s - loss: 691.7528 - val_loss: 688.0318\n",
      "Epoch 32/1000\n",
      "37605/37605 [==============================] - 7s - loss: 691.5786 - val_loss: 688.3432\n",
      "Epoch 33/1000\n",
      "37605/37605 [==============================] - 7s - loss: 691.3047 - val_loss: 687.4716\n",
      "Epoch 34/1000\n",
      "37605/37605 [==============================] - 7s - loss: 691.1109 - val_loss: 687.7943\n",
      "Epoch 35/1000\n",
      "37605/37605 [==============================] - 7s - loss: 691.0694 - val_loss: 687.1259\n",
      "Epoch 36/1000\n",
      "37605/37605 [==============================] - 7s - loss: 690.8237 - val_loss: 686.9269\n",
      "Epoch 37/1000\n",
      "37605/37605 [==============================] - 7s - loss: 690.5288 - val_loss: 687.0604\n",
      "Epoch 38/1000\n",
      "37605/37605 [==============================] - 7s - loss: 690.4910 - val_loss: 686.9285\n",
      "Epoch 39/1000\n",
      "37605/37605 [==============================] - 7s - loss: 690.3248 - val_loss: 686.7738\n",
      "Epoch 40/1000\n",
      "37605/37605 [==============================] - 7s - loss: 690.0457 - val_loss: 686.5443\n",
      "Epoch 41/1000\n",
      "37605/37605 [==============================] - 7s - loss: 689.8396 - val_loss: 686.4545\n",
      "Epoch 42/1000\n",
      "37605/37605 [==============================] - 7s - loss: 689.5193 - val_loss: 686.0929\n",
      "Epoch 43/1000\n",
      "37605/37605 [==============================] - 7s - loss: 689.4200 - val_loss: 686.2185\n",
      "Epoch 44/1000\n",
      "37605/37605 [==============================] - 7s - loss: 689.3086 - val_loss: 685.8209\n",
      "Epoch 45/1000\n",
      "37605/37605 [==============================] - 7s - loss: 689.1566 - val_loss: 685.9104\n",
      "Epoch 46/1000\n",
      "37605/37605 [==============================] - 7s - loss: 688.9547 - val_loss: 685.5038\n",
      "Epoch 47/1000\n",
      "37605/37605 [==============================] - 7s - loss: 688.6828 - val_loss: 685.4882\n",
      "Epoch 48/1000\n",
      "37605/37605 [==============================] - 7s - loss: 688.5229 - val_loss: 685.5654\n",
      "Epoch 49/1000\n",
      "37605/37605 [==============================] - 7s - loss: 688.4985 - val_loss: 685.4980\n",
      "Epoch 50/1000\n",
      "37605/37605 [==============================] - 7s - loss: 688.3150 - val_loss: 685.4541\n",
      "Epoch 51/1000\n",
      "37605/37605 [==============================] - 7s - loss: 688.1611 - val_loss: 684.9086\n",
      "Epoch 52/1000\n",
      "37605/37605 [==============================] - 7s - loss: 687.9016 - val_loss: 684.9892\n",
      "Epoch 53/1000\n",
      "37605/37605 [==============================] - 7s - loss: 687.8225 - val_loss: 685.0550\n",
      "Epoch 54/1000\n",
      "37605/37605 [==============================] - 7s - loss: 687.6113 - val_loss: 684.8105\n",
      "Epoch 55/1000\n",
      "37605/37605 [==============================] - 7s - loss: 687.5306 - val_loss: 684.6402\n",
      "Epoch 56/1000\n",
      "37605/37605 [==============================] - 7s - loss: 687.3695 - val_loss: 684.4136\n",
      "Epoch 57/1000\n",
      "37605/37605 [==============================] - 7s - loss: 687.1790 - val_loss: 684.4330\n",
      "Epoch 58/1000\n",
      "37605/37605 [==============================] - 7s - loss: 687.0014 - val_loss: 684.3079\n",
      "Epoch 59/1000\n",
      "37605/37605 [==============================] - 7s - loss: 686.8043 - val_loss: 684.4803\n",
      "Epoch 60/1000\n",
      "37605/37605 [==============================] - 7s - loss: 686.6652 - val_loss: 684.3206\n",
      "Epoch 61/1000\n",
      "37605/37605 [==============================] - 7s - loss: 686.6168 - val_loss: 683.9706\n",
      "Epoch 62/1000\n",
      "37605/37605 [==============================] - 7s - loss: 686.4495 - val_loss: 683.9140\n",
      "Epoch 63/1000\n",
      "37605/37605 [==============================] - 7s - loss: 686.2896 - val_loss: 684.2862\n",
      "Epoch 64/1000\n",
      "37605/37605 [==============================] - 7s - loss: 686.2018 - val_loss: 683.7315\n",
      "Epoch 65/1000\n",
      "37605/37605 [==============================] - 7s - loss: 686.0105 - val_loss: 683.5776\n",
      "Epoch 66/1000\n",
      "37605/37605 [==============================] - 7s - loss: 685.7655 - val_loss: 683.4595\n",
      "Epoch 67/1000\n",
      "37605/37605 [==============================] - 7s - loss: 685.7014 - val_loss: 683.4967\n",
      "Epoch 68/1000\n",
      "37605/37605 [==============================] - 7s - loss: 685.5208 - val_loss: 683.3347\n",
      "Epoch 69/1000\n",
      "37605/37605 [==============================] - 7s - loss: 685.5174 - val_loss: 683.4315\n",
      "Epoch 70/1000\n",
      "37605/37605 [==============================] - 7s - loss: 685.3503 - val_loss: 683.4243\n",
      "Epoch 71/1000\n",
      "37605/37605 [==============================] - 7s - loss: 685.1763 - val_loss: 683.1753\n",
      "Epoch 72/1000\n",
      "37605/37605 [==============================] - 7s - loss: 685.0013 - val_loss: 682.8531\n",
      "Epoch 73/1000\n",
      "37605/37605 [==============================] - 7s - loss: 685.0796 - val_loss: 683.0286\n",
      "Epoch 74/1000\n",
      "37605/37605 [==============================] - 7s - loss: 684.6928 - val_loss: 682.7439\n",
      "Epoch 75/1000\n",
      "37605/37605 [==============================] - 7s - loss: 684.6658 - val_loss: 682.5032\n",
      "Epoch 76/1000\n",
      "37605/37605 [==============================] - 7s - loss: 684.5594 - val_loss: 682.7337\n",
      "Epoch 77/1000\n",
      "37605/37605 [==============================] - 7s - loss: 684.3003 - val_loss: 682.4639\n",
      "Epoch 78/1000\n",
      "37605/37605 [==============================] - 7s - loss: 684.3016 - val_loss: 682.5890\n",
      "Epoch 79/1000\n",
      "37605/37605 [==============================] - 7s - loss: 684.0441 - val_loss: 681.9257\n",
      "Epoch 80/1000\n",
      "37605/37605 [==============================] - 7s - loss: 683.9736 - val_loss: 682.3609\n",
      "Epoch 81/1000\n",
      "37605/37605 [==============================] - 7s - loss: 683.9193 - val_loss: 682.1087\n",
      "Epoch 82/1000\n",
      "37605/37605 [==============================] - 7s - loss: 683.7361 - val_loss: 682.1219\n",
      "Epoch 83/1000\n",
      "37605/37605 [==============================] - 7s - loss: 683.6496 - val_loss: 682.1389\n",
      "Epoch 00082: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3090e85890>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "patience = 3\n",
    "earlyStopping = callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='min')\n",
    "\n",
    "vae.fit(train_Xtf,  \n",
    "        train_Xtf, \n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1, \n",
    "        callbacks=[earlyStopping], \n",
    "        validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1767, 5000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = cohen_vec.X\n",
    "cohen_X_tf = np.zeros((X.shape[0], vec.vocab_size))\n",
    "for i in range(len(X)) :\n",
    "    cohen_X_tf[i, X[i, :]] = 1.\n",
    "\n",
    "cohen_X_tf = cohen_X_tf[:, 2:]\n",
    "cohen_X_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedds = encoder.predict(cohen_X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('../data/files/test_cohen_dedup.csv')\n",
    "\n",
    "nb_studies = len(df)\n",
    "H = np.zeros((nb_studies, nb_studies))\n",
    "\n",
    "cdnos = list(set(df.cdno))\n",
    "for i in range(nb_studies) :\n",
    "    H[i, df[df['cdno'] == df['cdno'][i]].index] = 1\n",
    "    \n",
    "H[np.arange(nb_studies), np.arange(nb_studies)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "embedds_n = normalize(embedds, 'l2')\n",
    "scores = np.dot(embedds_n, embedds_n.T)\n",
    "scores[np.arange(nb_studies), np.arange(nb_studies)] = -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824494652055\n",
      "{'NSAIDS_processed': 0.8878757023735081, 'ACEInhibitors_processed': 0.82898028932750711, 'SkeletalMuscleRelaxants_processed': 0.71413229709440951, 'Triptans_processed': 0.90845402091448257, 'OralHypoglycemics_processed': 0.87597693482589933, 'CalciumChannelBlockers_processed': 0.67077607620547286, 'BetaBlockers_processed': 0.73342038663070841, 'Estrogens_processed': 0.91742778917335122, 'ADHD_processed': 0.90444448730766458, 'Statins_processed': 0.82841237909564536, 'Antihistamines_processed': 0.72860533391663285, 'ProtonPumpInhibitors_processed': 0.82000178720794759, 'Opiods_processed': 0.82089058991342667, 'AtypicalAntipsychotics_processed': 0.8793452344413768}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "aucs = [0] * nb_studies\n",
    "for i in range(nb_studies) :\n",
    "    aucs[i] = roc_auc_score(H[i], scores[i])\n",
    "print np.mean(aucs)\n",
    "rocs = {}\n",
    "for cd in cdnos :\n",
    "    idxs = df[df.cdno == cd].index\n",
    "    rocs[cd] = np.mean(np.array(aucs)[idxs])\n",
    "print rocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "dic = eval(\"{'NSAIDS_processed': 0.8878757023735081, 'ACEInhibitors_processed': 0.82898028932750711, 'SkeletalMuscleRelaxants_processed': 0.71413229709440951, 'Triptans_processed': 0.90845402091448257, 'OralHypoglycemics_processed': 0.87597693482589933, 'CalciumChannelBlockers_processed': 0.67077607620547286, 'BetaBlockers_processed': 0.73342038663070841, 'Estrogens_processed': 0.91742778917335122, 'ADHD_processed': 0.90444448730766458, 'Statins_processed': 0.82841237909564536, 'Antihistamines_processed': 0.72860533391663285, 'ProtonPumpInhibitors_processed': 0.82000178720794759, 'Opiods_processed': 0.82089058991342667, 'AtypicalAntipsychotics_processed': 0.8793452344413768}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACEInhibitors_processed              0.828980\n",
       "ADHD_processed                       0.904444\n",
       "Antihistamines_processed             0.728605\n",
       "AtypicalAntipsychotics_processed     0.879345\n",
       "BetaBlockers_processed               0.733420\n",
       "CalciumChannelBlockers_processed     0.670776\n",
       "Estrogens_processed                  0.917428\n",
       "NSAIDS_processed                     0.887876\n",
       "Opiods_processed                     0.820891\n",
       "OralHypoglycemics_processed          0.875977\n",
       "ProtonPumpInhibitors_processed       0.820002\n",
       "SkeletalMuscleRelaxants_processed    0.714132\n",
       "Statins_processed                    0.828412\n",
       "Triptans_processed                   0.908454\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

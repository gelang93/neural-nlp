{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import sys\n",
    "sys.path.insert(0, '../../preprocess')\n",
    "import vectorizer\n",
    "\n",
    "vec = cPickle.load(open('../../preprocess/allfields_with_embedding_19995.p', 'rb'))\n",
    "cohen_vec = cPickle.load(open('../../preprocess/cohendata_dedup_19995.p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = vec.index['abstract']\n",
    "vec.X = vec.X[index[0]:index[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_X = vec.X\n",
    "X_tf = np.zeros((train_X.shape[0], vec.vocab_size))\n",
    "for i in range(len(train_X)) :\n",
    "    X_tf[i, train_X[i, :]] = 1.\n",
    "\n",
    "X_tf = X_tf[:, 2:]\n",
    "train_Xtf = X_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 5e-5\n",
    "batch_size = 64\n",
    "vocab_size = vec.vocab_size - 2\n",
    "intermediate_dim = 500\n",
    "latent_dim = 200\n",
    "epochs = 1000\n",
    "epsilon_std = 1.0\n",
    "activation = 'tanh'\n",
    "\n",
    "x = Input(shape=(vocab_size,), name='x')\n",
    "h = Dense(intermediate_dim, activation=activation, name='h')(x)\n",
    "mu = Dense(latent_dim, name='mu')(h)\n",
    "log_sigma2 = Dense(latent_dim, name='l')(h)\n",
    "encoder = Model(x, mu)\n",
    "\n",
    "# reparameterized sampler for normal distributions\n",
    "def sample_norm(args):\n",
    "    '''reparameterized sampling from normal distribution'''\n",
    "    mu, log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(mu)[0], latent_dim,), mean=0.)\n",
    "    return mu + K.exp(0.5 * log_var) * epsilon\n",
    "\n",
    "# decoder / generative network\n",
    "z = Lambda(sample_norm, output_shape=(latent_dim,), name='z')([mu, log_sigma2])\n",
    "e = Dense(vocab_size, name='e')(z)\n",
    "\n",
    "def log_softmax(x, axis=None):\n",
    "    x0 = x - K.max(x, axis=axis, keepdims=True)\n",
    "    log_sum_exp_x0 = K.log(K.sum(K.exp(x0), axis=axis, keepdims=True))\n",
    "    return x0 - log_sum_exp_x0\n",
    "\n",
    "def kl_loss(x, e): \n",
    "    return (- 0.5 * K.sum(1 + log_sigma2 - K.square(mu) - K.exp(log_sigma2), axis=-1))\n",
    "\n",
    "\n",
    "def cross_ent_loss(x, e): \n",
    "    return - K.sum(x * log_softmax(e, axis=-1), axis=-1) \n",
    "    \n",
    "\n",
    "def vae_loss(x, e):\n",
    "    xent_loss = cross_ent_loss(x, e)\n",
    "    kld = kl_loss(x, e)\n",
    "    return xent_loss + kld\n",
    "\n",
    "\n",
    "opt = optimizers.adam(lr=learning_rate)\n",
    "vae = Model(x, e)\n",
    "vae.compile(optimizer=opt, \n",
    "            loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37605 samples, validate on 4179 samples\n",
      "Epoch 1/1000\n",
      "37605/37605 [==============================] - 17s 464us/step - loss: 992.7396 - val_loss: 891.7777\n",
      "Epoch 2/1000\n",
      "37605/37605 [==============================] - 17s 452us/step - loss: 888.5943 - val_loss: 868.4057\n",
      "Epoch 3/1000\n",
      "37605/37605 [==============================] - 18s 468us/step - loss: 874.8702 - val_loss: 860.4516\n",
      "Epoch 4/1000\n",
      "37605/37605 [==============================] - 18s 471us/step - loss: 868.8014 - val_loss: 856.2373\n",
      "Epoch 5/1000\n",
      "37605/37605 [==============================] - 18s 471us/step - loss: 865.3292 - val_loss: 853.2794\n",
      "Epoch 6/1000\n",
      "37605/37605 [==============================] - 18s 470us/step - loss: 862.7563 - val_loss: 851.4342\n",
      "Epoch 7/1000\n",
      "37605/37605 [==============================] - 18s 465us/step - loss: 860.5509 - val_loss: 848.9236\n",
      "Epoch 8/1000\n",
      "37605/37605 [==============================] - 17s 461us/step - loss: 858.1830 - val_loss: 847.2568\n",
      "Epoch 9/1000\n",
      "37605/37605 [==============================] - 18s 474us/step - loss: 856.1912 - val_loss: 845.3637\n",
      "Epoch 10/1000\n",
      "37605/37605 [==============================] - 18s 465us/step - loss: 854.1998 - val_loss: 843.5783\n",
      "Epoch 11/1000\n",
      "37605/37605 [==============================] - 17s 452us/step - loss: 852.3779 - val_loss: 842.0011\n",
      "Epoch 12/1000\n",
      "37605/37605 [==============================] - 18s 469us/step - loss: 850.6931 - val_loss: 840.6591\n",
      "Epoch 13/1000\n",
      "37605/37605 [==============================] - 18s 468us/step - loss: 848.9079 - val_loss: 839.7285\n",
      "Epoch 14/1000\n",
      "37605/37605 [==============================] - 18s 469us/step - loss: 847.3656 - val_loss: 837.9082\n",
      "Epoch 15/1000\n",
      "37605/37605 [==============================] - 18s 468us/step - loss: 846.0634 - val_loss: 836.9662\n",
      "Epoch 16/1000\n",
      "37605/37605 [==============================] - 18s 475us/step - loss: 844.6608 - val_loss: 835.9156\n",
      "Epoch 17/1000\n",
      "37605/37605 [==============================] - 17s 458us/step - loss: 843.3893 - val_loss: 834.9220\n",
      "Epoch 18/1000\n",
      "37605/37605 [==============================] - 17s 464us/step - loss: 842.2973 - val_loss: 833.8981\n",
      "Epoch 19/1000\n",
      "37605/37605 [==============================] - 17s 452us/step - loss: 841.1322 - val_loss: 832.9455\n",
      "Epoch 20/1000\n",
      "37605/37605 [==============================] - 17s 462us/step - loss: 840.0365 - val_loss: 832.0030\n",
      "Epoch 21/1000\n",
      "37605/37605 [==============================] - 17s 447us/step - loss: 839.0620 - val_loss: 831.0131\n",
      "Epoch 22/1000\n",
      "37605/37605 [==============================] - 17s 456us/step - loss: 838.1328 - val_loss: 830.3047\n",
      "Epoch 23/1000\n",
      "37605/37605 [==============================] - 17s 446us/step - loss: 837.1782 - val_loss: 830.4757\n",
      "Epoch 24/1000\n",
      "37605/37605 [==============================] - 17s 453us/step - loss: 836.3346 - val_loss: 828.9633\n",
      "Epoch 25/1000\n",
      "37605/37605 [==============================] - 17s 451us/step - loss: 835.4366 - val_loss: 828.1224\n",
      "Epoch 26/1000\n",
      "37605/37605 [==============================] - 17s 447us/step - loss: 834.4982 - val_loss: 827.4399\n",
      "Epoch 27/1000\n",
      "37605/37605 [==============================] - 17s 444us/step - loss: 833.9284 - val_loss: 827.4449\n",
      "Epoch 28/1000\n",
      "37605/37605 [==============================] - 17s 449us/step - loss: 832.9324 - val_loss: 826.5575\n",
      "Epoch 29/1000\n",
      "37605/37605 [==============================] - 17s 447us/step - loss: 832.2757 - val_loss: 826.1771\n",
      "Epoch 30/1000\n",
      "37605/37605 [==============================] - 17s 449us/step - loss: 831.6496 - val_loss: 826.1051\n",
      "Epoch 31/1000\n",
      "37605/37605 [==============================] - 17s 443us/step - loss: 830.9494 - val_loss: 824.6894\n",
      "Epoch 32/1000\n",
      "37605/37605 [==============================] - 17s 445us/step - loss: 830.2105 - val_loss: 824.2339\n",
      "Epoch 33/1000\n",
      "37605/37605 [==============================] - 17s 450us/step - loss: 829.4302 - val_loss: 823.5888\n",
      "Epoch 34/1000\n",
      "37605/37605 [==============================] - 17s 447us/step - loss: 828.7221 - val_loss: 823.5853\n",
      "Epoch 35/1000\n",
      "37605/37605 [==============================] - 17s 448us/step - loss: 828.0650 - val_loss: 822.9868\n",
      "Epoch 36/1000\n",
      "37605/37605 [==============================] - 17s 440us/step - loss: 827.6540 - val_loss: 822.7625\n",
      "Epoch 37/1000\n",
      "37605/37605 [==============================] - 17s 443us/step - loss: 827.0617 - val_loss: 822.3540\n",
      "Epoch 38/1000\n",
      "37605/37605 [==============================] - 17s 443us/step - loss: 826.3882 - val_loss: 821.4378\n",
      "Epoch 39/1000\n",
      "37605/37605 [==============================] - 17s 441us/step - loss: 825.7466 - val_loss: 821.2995\n",
      "Epoch 40/1000\n",
      "37605/37605 [==============================] - 17s 441us/step - loss: 825.2337 - val_loss: 820.8265\n",
      "Epoch 41/1000\n",
      "37605/37605 [==============================] - 17s 447us/step - loss: 824.6685 - val_loss: 820.7738\n",
      "Epoch 42/1000\n",
      "37605/37605 [==============================] - 16s 434us/step - loss: 824.1495 - val_loss: 820.1989\n",
      "Epoch 43/1000\n",
      "37605/37605 [==============================] - 16s 437us/step - loss: 823.6852 - val_loss: 819.8592\n",
      "Epoch 44/1000\n",
      "37605/37605 [==============================] - 17s 442us/step - loss: 823.0867 - val_loss: 819.3095\n",
      "Epoch 45/1000\n",
      "37605/37605 [==============================] - 17s 443us/step - loss: 822.5993 - val_loss: 818.9638\n",
      "Epoch 46/1000\n",
      "37605/37605 [==============================] - 17s 444us/step - loss: 822.1572 - val_loss: 818.9947\n",
      "Epoch 47/1000\n",
      "37605/37605 [==============================] - 17s 447us/step - loss: 821.5127 - val_loss: 818.8208\n",
      "Epoch 48/1000\n",
      "37605/37605 [==============================] - 16s 436us/step - loss: 821.1077 - val_loss: 818.4717\n",
      "Epoch 49/1000\n",
      "37605/37605 [==============================] - 17s 441us/step - loss: 820.5501 - val_loss: 817.6427\n",
      "Epoch 50/1000\n",
      "37605/37605 [==============================] - 18s 491us/step - loss: 820.0538 - val_loss: 816.9263\n",
      "Epoch 51/1000\n",
      "37605/37605 [==============================] - 19s 513us/step - loss: 819.7391 - val_loss: 817.1391\n",
      "Epoch 52/1000\n",
      "37605/37605 [==============================] - 19s 508us/step - loss: 819.3723 - val_loss: 817.1600\n",
      "Epoch 53/1000\n",
      "37605/37605 [==============================] - 19s 516us/step - loss: 818.7202 - val_loss: 816.5795\n",
      "Epoch 54/1000\n",
      "37605/37605 [==============================] - 19s 512us/step - loss: 818.3160 - val_loss: 816.5670\n",
      "Epoch 55/1000\n",
      "37605/37605 [==============================] - 19s 515us/step - loss: 817.8665 - val_loss: 815.7090\n",
      "Epoch 56/1000\n",
      "37605/37605 [==============================] - 19s 515us/step - loss: 817.6339 - val_loss: 815.4489\n",
      "Epoch 57/1000\n",
      "37605/37605 [==============================] - 19s 516us/step - loss: 817.1376 - val_loss: 815.6801\n",
      "Epoch 58/1000\n",
      "37605/37605 [==============================] - 19s 518us/step - loss: 816.7395 - val_loss: 814.9270\n",
      "Epoch 59/1000\n",
      "37605/37605 [==============================] - 19s 512us/step - loss: 816.2706 - val_loss: 814.7566\n",
      "Epoch 60/1000\n",
      "37605/37605 [==============================] - 19s 513us/step - loss: 815.9394 - val_loss: 814.5881\n",
      "Epoch 61/1000\n",
      "37605/37605 [==============================] - 19s 515us/step - loss: 815.5596 - val_loss: 814.8554\n",
      "Epoch 62/1000\n",
      "37605/37605 [==============================] - 19s 508us/step - loss: 815.2622 - val_loss: 814.1267\n",
      "Epoch 63/1000\n",
      "37605/37605 [==============================] - 19s 510us/step - loss: 814.7945 - val_loss: 814.0392\n",
      "Epoch 64/1000\n",
      "37605/37605 [==============================] - 19s 509us/step - loss: 814.4881 - val_loss: 814.1373\n",
      "Epoch 65/1000\n",
      "37605/37605 [==============================] - 19s 506us/step - loss: 814.0147 - val_loss: 813.1117\n",
      "Epoch 66/1000\n",
      "37605/37605 [==============================] - 18s 491us/step - loss: 813.6910 - val_loss: 813.4155\n",
      "Epoch 67/1000\n",
      "37605/37605 [==============================] - 18s 490us/step - loss: 813.3904 - val_loss: 813.2636\n",
      "Epoch 68/1000\n",
      "37605/37605 [==============================] - 18s 473us/step - loss: 813.0510 - val_loss: 812.9952\n",
      "Epoch 69/1000\n",
      "37605/37605 [==============================] - 18s 470us/step - loss: 812.6052 - val_loss: 812.5787\n",
      "Epoch 70/1000\n",
      "37605/37605 [==============================] - 17s 453us/step - loss: 812.2915 - val_loss: 812.3679\n",
      "Epoch 71/1000\n",
      "37605/37605 [==============================] - 17s 455us/step - loss: 811.9083 - val_loss: 812.0457\n",
      "Epoch 72/1000\n",
      "37605/37605 [==============================] - 17s 458us/step - loss: 811.6367 - val_loss: 811.6954\n",
      "Epoch 73/1000\n",
      "37605/37605 [==============================] - 17s 456us/step - loss: 811.3641 - val_loss: 811.8783\n",
      "Epoch 74/1000\n",
      "37605/37605 [==============================] - 17s 459us/step - loss: 810.9740 - val_loss: 812.2997\n",
      "Epoch 75/1000\n",
      "37605/37605 [==============================] - 17s 446us/step - loss: 810.8069 - val_loss: 811.4024\n",
      "Epoch 76/1000\n",
      "37605/37605 [==============================] - 17s 444us/step - loss: 810.3687 - val_loss: 810.8709\n",
      "Epoch 77/1000\n",
      "37605/37605 [==============================] - 17s 457us/step - loss: 810.1048 - val_loss: 810.8770\n",
      "Epoch 78/1000\n",
      "37605/37605 [==============================] - 17s 443us/step - loss: 809.7591 - val_loss: 811.0623\n",
      "Epoch 79/1000\n",
      "37605/37605 [==============================] - 17s 451us/step - loss: 809.4292 - val_loss: 811.2432\n",
      "Epoch 00079: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3931346d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "patience = 3\n",
    "earlyStopping = callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='min')\n",
    "\n",
    "vae.fit(train_Xtf,  \n",
    "        train_Xtf, \n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1, \n",
    "        callbacks=[earlyStopping], \n",
    "        validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1767, 19993)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = cohen_vec.X\n",
    "cohen_X_tf = np.zeros((X.shape[0], vec.vocab_size))\n",
    "for i in range(len(X)) :\n",
    "    cohen_X_tf[i, X[i, :]] = 1.\n",
    "\n",
    "cohen_X_tf = cohen_X_tf[:, 2:]\n",
    "cohen_X_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedds = encoder.predict(cohen_X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('../data/files/test_cohen_dedup.csv')\n",
    "\n",
    "nb_studies = len(df)\n",
    "H = np.zeros((nb_studies, nb_studies))\n",
    "\n",
    "cdnos = list(set(df.cdno))\n",
    "for i in range(nb_studies) :\n",
    "    H[i, df[df['cdno'] == df['cdno'][i]].index] = 1\n",
    "    \n",
    "H[np.arange(nb_studies), np.arange(nb_studies)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "embedds_n = normalize(embedds, 'l2')\n",
    "scores = np.dot(embedds_n, embedds_n.T)\n",
    "scores[np.arange(nb_studies), np.arange(nb_studies)] = -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849729726559\n",
      "ACEInhibitors_processed              0.854241\n",
      "ADHD_processed                       0.926045\n",
      "Antihistamines_processed             0.792442\n",
      "AtypicalAntipsychotics_processed     0.890929\n",
      "BetaBlockers_processed               0.760706\n",
      "CalciumChannelBlockers_processed     0.695967\n",
      "Estrogens_processed                  0.938406\n",
      "NSAIDS_processed                     0.899486\n",
      "Opiods_processed                     0.834546\n",
      "OralHypoglycemics_processed          0.886663\n",
      "ProtonPumpInhibitors_processed       0.853019\n",
      "SkeletalMuscleRelaxants_processed    0.754179\n",
      "Statins_processed                    0.870676\n",
      "Triptans_processed                   0.923441\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "aucs = [0] * nb_studies\n",
    "for i in range(nb_studies) :\n",
    "    aucs[i] = roc_auc_score(H[i], scores[i])\n",
    "print np.mean(aucs)\n",
    "rocs = {}\n",
    "for cd in cdnos :\n",
    "    idxs = df[df.cdno == cd].index\n",
    "    rocs[cd] = np.mean(np.array(aucs)[idxs])\n",
    "print pd.Series(rocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACEInhibitors_processed              0.828980\n",
       "ADHD_processed                       0.904444\n",
       "Antihistamines_processed             0.728605\n",
       "AtypicalAntipsychotics_processed     0.879345\n",
       "BetaBlockers_processed               0.733420\n",
       "CalciumChannelBlockers_processed     0.670776\n",
       "Estrogens_processed                  0.917428\n",
       "NSAIDS_processed                     0.887876\n",
       "Opiods_processed                     0.820891\n",
       "OralHypoglycemics_processed          0.875977\n",
       "ProtonPumpInhibitors_processed       0.820002\n",
       "SkeletalMuscleRelaxants_processed    0.714132\n",
       "Statins_processed                    0.828412\n",
       "Triptans_processed                   0.908454\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
